{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T02:18:32.375407795Z",
     "start_time": "2023-10-04T02:18:29.723460046Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 21:18:29.776735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-03 21:18:30.013055: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2023-10-03 21:18:30.013073: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-03 21:18:30.842025: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2023-10-03 21:18:30.842086: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2023-10-03 21:18:30.842092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/kaleb/Documents/eeg_dataset/files/S001'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 19\u001B[0m\n\u001B[1;32m     11\u001B[0m label_mapping \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;241m1\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRest\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;241m2\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSqueeze Both Fists\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;241m5\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSqueeze Right Hand\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m     }\n\u001B[1;32m     18\u001B[0m num_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m---> 19\u001B[0m X, Y \u001B[38;5;241m=\u001B[39m \u001B[43mread_edf_files\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreader\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/read_edf_files.py:24\u001B[0m, in \u001B[0;36mreader\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m label_mapping \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m1\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRest\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m2\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSqueeze Both Fists\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;241m5\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSqueeze Right Hand\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     21\u001B[0m }\n\u001B[1;32m     23\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/home/kaleb/Documents/eeg_dataset/files/S001\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 24\u001B[0m all_files \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m edf_files \u001B[38;5;241m=\u001B[39m [file \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m all_files \u001B[38;5;28;01mif\u001B[39;00m file\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.edf\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# if the file is not one of interest, continue (1, 2, 4, 6, 8, 10, 12, 14)\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/kaleb/Documents/eeg_dataset/files/S001'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pyedflib\n",
    "from utils.EEGModels import EEGNet\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import read_edf_files\n",
    "kernels, chans = 1, 64\n",
    "label_mapping = {\n",
    "        1: \"Rest\",\n",
    "        2: \"Squeeze Both Fists\",\n",
    "        3: \"Squeeze Both Feet\",\n",
    "        4: \"Squeeze Left Hand\",\n",
    "        5: \"Squeeze Right Hand\",\n",
    "    }\n",
    "num_labels = 5\n",
    "X, Y = read_edf_files.reader() # use other function to read the edf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (90, 64, 656, 1) \n",
      "y_train shape:  (90, 5)\n",
      "test print statement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 22:14:08.053958: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-02 22:14:08.053987: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-02 22:14:08.054005: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kaleb-School): /proc/driver/nvidia/version does not exist\n",
      "2023-10-02 22:14:08.054282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.56743, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 3s - loss: 1.6126 - accuracy: 0.3222 - val_loss: 1.5674 - val_accuracy: 0.3556 - 3s/epoch - 434ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.56743 to 1.53882, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 1.4426 - accuracy: 0.5222 - val_loss: 1.5388 - val_accuracy: 0.4667 - 1s/epoch - 217ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.53882 to 1.51116, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 1.3263 - accuracy: 0.5889 - val_loss: 1.5112 - val_accuracy: 0.5111 - 1s/epoch - 221ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.51116 to 1.48026, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 1.2660 - accuracy: 0.6333 - val_loss: 1.4803 - val_accuracy: 0.5333 - 1s/epoch - 219ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.48026 to 1.45718, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 1.1820 - accuracy: 0.6333 - val_loss: 1.4572 - val_accuracy: 0.5556 - 1s/epoch - 218ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.45718 to 1.43528, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 1.1216 - accuracy: 0.6667 - val_loss: 1.4353 - val_accuracy: 0.5556 - 1s/epoch - 233ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.43528 to 1.41635, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 1.0471 - accuracy: 0.6889 - val_loss: 1.4163 - val_accuracy: 0.5778 - 1s/epoch - 212ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.41635 to 1.38817, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 1.0183 - accuracy: 0.6778 - val_loss: 1.3882 - val_accuracy: 0.5778 - 1s/epoch - 217ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.38817 to 1.35970, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.9699 - accuracy: 0.7000 - val_loss: 1.3597 - val_accuracy: 0.6000 - 1s/epoch - 213ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.35970 to 1.34418, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.9483 - accuracy: 0.7333 - val_loss: 1.3442 - val_accuracy: 0.6000 - 1s/epoch - 222ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.34418 to 1.33207, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.8985 - accuracy: 0.7333 - val_loss: 1.3321 - val_accuracy: 0.6000 - 1s/epoch - 214ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 1.33207 to 1.33138, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.8488 - accuracy: 0.8000 - val_loss: 1.3314 - val_accuracy: 0.6444 - 1s/epoch - 210ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 1.33138 to 1.32262, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.8420 - accuracy: 0.7444 - val_loss: 1.3226 - val_accuracy: 0.6444 - 1s/epoch - 214ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 1.32262 to 1.31109, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.8268 - accuracy: 0.8000 - val_loss: 1.3111 - val_accuracy: 0.6667 - 1s/epoch - 237ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 1.31109 to 1.28756, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.8112 - accuracy: 0.7778 - val_loss: 1.2876 - val_accuracy: 0.6222 - 1s/epoch - 240ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 1.28756 to 1.27435, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.8032 - accuracy: 0.7778 - val_loss: 1.2744 - val_accuracy: 0.6222 - 2s/epoch - 262ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 1.27435 to 1.26031, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.7747 - accuracy: 0.8000 - val_loss: 1.2603 - val_accuracy: 0.6444 - 1s/epoch - 217ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 1.26031 to 1.24417, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.7158 - accuracy: 0.8667 - val_loss: 1.2442 - val_accuracy: 0.6444 - 1s/epoch - 221ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 1.24417 to 1.23828, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.6943 - accuracy: 0.8667 - val_loss: 1.2383 - val_accuracy: 0.6444 - 1s/epoch - 247ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 1.23828 to 1.23765, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.7227 - accuracy: 0.8444 - val_loss: 1.2376 - val_accuracy: 0.6444 - 1s/epoch - 224ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 1.23765 to 1.23383, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.7233 - accuracy: 0.8556 - val_loss: 1.2338 - val_accuracy: 0.6667 - 1s/epoch - 223ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 1.23383 to 1.21552, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.6975 - accuracy: 0.8778 - val_loss: 1.2155 - val_accuracy: 0.6222 - 1s/epoch - 224ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 1.21552 to 1.20770, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.6715 - accuracy: 0.9000 - val_loss: 1.2077 - val_accuracy: 0.6222 - 1s/epoch - 224ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 1.20770 to 1.20149, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.6579 - accuracy: 0.8667 - val_loss: 1.2015 - val_accuracy: 0.6444 - 1s/epoch - 217ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.20149\n",
      "6/6 - 1s - loss: 0.6449 - accuracy: 0.9000 - val_loss: 1.2020 - val_accuracy: 0.6667 - 1s/epoch - 217ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 1.20149 to 1.18700, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.6301 - accuracy: 0.8667 - val_loss: 1.1870 - val_accuracy: 0.6222 - 1s/epoch - 228ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.18700\n",
      "6/6 - 1s - loss: 0.5936 - accuracy: 0.8778 - val_loss: 1.1870 - val_accuracy: 0.6222 - 1s/epoch - 225ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 1.18700 to 1.17309, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.5740 - accuracy: 0.9111 - val_loss: 1.1731 - val_accuracy: 0.6444 - 1s/epoch - 232ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 1.17309 to 1.16326, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 1s - loss: 0.5951 - accuracy: 0.9333 - val_loss: 1.1633 - val_accuracy: 0.6444 - 1s/epoch - 227ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.16326\n",
      "6/6 - 1s - loss: 0.5939 - accuracy: 0.9111 - val_loss: 1.1698 - val_accuracy: 0.6222 - 1s/epoch - 217ms/step\n",
      "2/2 [==============================] - 0s 22ms/step\n",
      "Classification accuracy: 0.555556 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 22:14:50.309040: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 42991616 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "## Process, filter, and epoch the data\n",
    "# init arrays to train/validate/test. Make split 50/25/25\n",
    "half = int(len(X) / 2)\n",
    "quarter = int(half / 2)\n",
    "three_fourths = half + quarter\n",
    "\n",
    "X_train = X[:half, :, :]\n",
    "X_validate = X[half : three_fourths, :, :]\n",
    "X_test = X[three_fourths:, :, :]\n",
    "\n",
    "y_train_int = Y[:half]\n",
    "y_validate_int = Y[half:three_fourths]\n",
    "y_test_int = Y[three_fourths:]\n",
    "\n",
    "# convert labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train_int-1)\n",
    "y_validate = np_utils.to_categorical(y_validate_int-1)\n",
    "y_test = np_utils.to_categorical(y_test_int-1)\n",
    "\n",
    "# convert data to NHWC (trials, channels, samples, kernels) format\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], kernels)\n",
    "X_validate = X_validate.reshape(X_validate.shape[0], X_validate.shape[1], X_validate.shape[2], kernels)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], kernels)\n",
    "\n",
    "print('x_train shape: ', X_train.shape, '\\ny_train shape: ', y_train.shape)\n",
    "################################################################\n",
    "## Call EEGNet\n",
    "\n",
    "model = EEGNet(nb_classes=num_labels, Chans=X_train.shape[1], Samples=X_train.shape[2],\n",
    "               dropoutRate=0.5, kernLength=32, F1=8, D=2, F2=16,\n",
    "                 dropoutType= 'Dropout')\n",
    "\n",
    "# compile the model and set the optimizers\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# count number of parameters in the model\n",
    "numParams    = model.count_params()    \n",
    "\n",
    "# set a valid path for your system to record model checkpoints\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "###############################################################################\n",
    "# if the classification task was imbalanced (significantly more trials in one\n",
    "# class versus the others) you can assign a weight to each class during \n",
    "# optimization to balance it out. This data is approximately balanced so we \n",
    "# don't need to do this, but is shown here for illustration/completeness. \n",
    "###############################################################################\n",
    "\n",
    "# the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
    "# the weights all to be 1\n",
    "class_weights = {0:1, 1:1, 2:1, 3:1, 4:1}\n",
    "\n",
    "################################################################################\n",
    "# fit the model. Due to very small sample sizes this can get\n",
    "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
    "# Riemannian geometry classification (below)\n",
    "################################################################################\n",
    "# fittedModel = model.fit(X_train, y_train, batch_size = 16, epochs = 30, \n",
    "#                         verbose = 2, validation_data=(X_validate, y_validate),\n",
    "#                        callbacks=[checkpointer], class_weight = class_weights)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size = 16, epochs = 30, \n",
    "          verbose = 2, validation_data=(X_validate, y_validate),\n",
    "          callbacks=[checkpointer], class_weight = class_weights)\n",
    "\n",
    "# load optimal weights\n",
    "model.load_weights('/tmp/checkpoint.h5')\n",
    "\n",
    "###############################################################################\n",
    "# can alternatively used the weights provided in the repo. If so it should get\n",
    "# you 93% accuracy. Change the WEIGHTS_PATH variable to wherever it is on your\n",
    "# system.\n",
    "###############################################################################\n",
    "\n",
    "# WEIGHTS_PATH = /path/to/EEGNet-8-2-weights.h5 \n",
    "# model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "###############################################################################\n",
    "# make prediction on test set.\n",
    "###############################################################################\n",
    "\n",
    "probs       = model.predict(X_test)\n",
    "preds       = probs.argmax(axis = -1)  \n",
    "acc         = np.mean(preds == y_test.argmax(axis=-1))\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb Cell 3\u001B[0m line \u001B[0;36m8\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39msklearn\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m shuffle\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39m# Assuming you have a trained model called 'model'\u001B[39;00m\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001B[0m \u001B[39m# X: EEG data, y: labels\u001B[39;00m\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001B[0m \n\u001B[1;32m      <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m \u001B[39m# Calculate accuracy on the original data\u001B[39;00m\n\u001B[0;32m----> <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m original_accuracy \u001B[39m=\u001B[39m acc \u001B[39m# accuracy_score(Y, model.predict(X).argmax(axis=1))\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m \u001B[39m# Initialize a list to store feature importances\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/feature_importance_1.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001B[0m feature_importances \u001B[39m=\u001B[39m []\n",
      "\u001B[0;31mNameError\u001B[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Assuming you have a trained model called 'model'\n",
    "# X: EEG data, y: labels\n",
    "\n",
    "# Calculate accuracy on the original data\n",
    "original_accuracy = acc # accuracy_score(Y, model.predict(X).argmax(axis=1))\n",
    "\n",
    "# Initialize a list to store feature importances\n",
    "feature_importances = []\n",
    "\n",
    "# Iterate through each electrode\n",
    "for electrode_idx in range(64):\n",
    "    # Create a copy of the original data\n",
    "    X_permuted = X.copy()\n",
    "    \n",
    "    # Permute the values of the current electrode\n",
    "    X_permuted[:, electrode_idx, :] = shuffle(X_permuted[:, electrode_idx, :])\n",
    "    print(X_permuted.shape)\n",
    "    print(X.shape)\n",
    "    exit()\n",
    "    \n",
    "    # Calculate accuracy on the permuted data\n",
    "    probs       = model.predict(X_permuted)\n",
    "    preds       = probs.argmax(axis = -1)  \n",
    "    permuted_accuracy = np.mean(preds == y_test.argmax(axis=-1))\n",
    "    # permuted_accuracy = accuracy_score(Y, model.predict(X_permuted).argmax(axis=1))\n",
    "    \n",
    "    # Calculate feature importance as the drop in accuracy\n",
    "    feature_importance = original_accuracy - permuted_accuracy\n",
    "    \n",
    "    # Append the feature importance score to the list\n",
    "    feature_importances.append(feature_importance)\n",
    "\n",
    "# Now 'feature_importances' list contains the importance scores for each electrode\n",
    "# You can associate these scores with corresponding electrode names/indexes for further analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
