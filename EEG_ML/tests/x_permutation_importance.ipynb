{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 19:35:51.733627: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 19:35:52.009525: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-02 19:35:52.009543: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-02 19:35:52.046914: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-02 19:35:53.261636: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-02 19:35:53.261725: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-02 19:35:53.261734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (180, 64, 656)\n",
      "Y.shape:  (180,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "'''\n",
    "import numpy as np\n",
    "import os\n",
    "import pyedflib\n",
    "from utils.EEGModels import EEGNet\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import read_edf_files\n",
    "kernels, chans = 1, 64\n",
    "label_mapping = {\n",
    "        1: \"Rest\",\n",
    "        2: \"Squeeze Both Fists\",\n",
    "        3: \"Squeeze Both Feet\",\n",
    "        4: \"Squeeze Left Hand\",\n",
    "        5: \"Squeeze Right Hand\",\n",
    "    }\n",
    "num_labels = 5\n",
    "X, Y = read_edf_files.reader() # use other function to read the edf files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (90, 64, 656, 1) \n",
      "y_train shape:  (90, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 19:35:57.862028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-02 19:35:57.862068: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-02 19:35:57.862096: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kaleb-School): /proc/driver/nvidia/version does not exist\n",
      "2023-10-02 19:35:57.862455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test print statement\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "## Process, filter, and epoch the data\n",
    "# init arrays to train/validate/test. Make split 50/25/25\n",
    "half = int(len(X) / 2)\n",
    "quarter = int(half / 2)\n",
    "three_fourths = half + quarter\n",
    "\n",
    "X_train = X[:half, :, :]\n",
    "X_validate = X[half : three_fourths, :, :]\n",
    "X_test = X[three_fourths:, :, :]\n",
    "\n",
    "y_train_int = Y[:half]\n",
    "y_validate_int = Y[half:three_fourths]\n",
    "y_test_int = Y[three_fourths:]\n",
    "\n",
    "# convert labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train_int-1)\n",
    "y_validate = np_utils.to_categorical(y_validate_int-1)\n",
    "y_test = np_utils.to_categorical(y_test_int-1)\n",
    "\n",
    "# convert data to NHWC (trials, channels, samples, kernels) format\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], kernels)\n",
    "X_validate = X_validate.reshape(X_validate.shape[0], X_validate.shape[1], X_validate.shape[2], kernels)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], kernels)\n",
    "\n",
    "print('x_train shape: ', X_train.shape, '\\ny_train shape: ', y_train.shape)\n",
    "################################################################\n",
    "## Call EEGNet\n",
    "\n",
    "model = EEGNet(nb_classes=num_labels, Chans=X_train.shape[1], Samples=X_train.shape[2],\n",
    "               dropoutRate=0.5, kernLength=32, F1=8, D=2, F2=16,\n",
    "                 dropoutType= 'Dropout')\n",
    "\n",
    "# compile the model and set the optimizers\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# count number of parameters in the model\n",
    "numParams    = model.count_params()    \n",
    "\n",
    "# set a valid path for your system to record model checkpoints\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "###############################################################################\n",
    "# if the classification task was imbalanced (significantly more trials in one\n",
    "# class versus the others) you can assign a weight to each class during \n",
    "# optimization to balance it out. This data is approximately balanced so we \n",
    "# don't need to do this, but is shown here for illustration/completeness. \n",
    "###############################################################################\n",
    "\n",
    "# the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
    "# the weights all to be 1\n",
    "class_weights = {0:1, 1:1, 2:1, 3:1, 4:1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.57136, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 4s - loss: 1.7437 - accuracy: 0.2444 - val_loss: 1.5714 - val_accuracy: 0.4222 - 4s/epoch - 622ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.57136 to 1.54208, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 1.4658 - accuracy: 0.5111 - val_loss: 1.5421 - val_accuracy: 0.5111 - 2s/epoch - 321ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.54208 to 1.51467, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 1.3487 - accuracy: 0.5778 - val_loss: 1.5147 - val_accuracy: 0.4889 - 2s/epoch - 344ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.51467 to 1.48712, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 1.2569 - accuracy: 0.6222 - val_loss: 1.4871 - val_accuracy: 0.5111 - 2s/epoch - 331ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.48712 to 1.47150, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 1.1863 - accuracy: 0.6222 - val_loss: 1.4715 - val_accuracy: 0.5556 - 2s/epoch - 330ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.47150 to 1.46003, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 1.1202 - accuracy: 0.6889 - val_loss: 1.4600 - val_accuracy: 0.5778 - 2s/epoch - 342ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.46003 to 1.44454, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 1.0731 - accuracy: 0.6889 - val_loss: 1.4445 - val_accuracy: 0.5778 - 2s/epoch - 310ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.44454 to 1.42580, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 1.0045 - accuracy: 0.6556 - val_loss: 1.4258 - val_accuracy: 0.5778 - 2s/epoch - 309ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.42580 to 1.41464, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.9837 - accuracy: 0.6889 - val_loss: 1.4146 - val_accuracy: 0.5778 - 2s/epoch - 310ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.41464 to 1.40593, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.9106 - accuracy: 0.7556 - val_loss: 1.4059 - val_accuracy: 0.5333 - 2s/epoch - 311ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.40593 to 1.39012, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.8627 - accuracy: 0.7444 - val_loss: 1.3901 - val_accuracy: 0.5556 - 2s/epoch - 330ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 1.39012 to 1.37880, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.8269 - accuracy: 0.7667 - val_loss: 1.3788 - val_accuracy: 0.5333 - 2s/epoch - 315ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 1.37880 to 1.36895, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.8045 - accuracy: 0.7778 - val_loss: 1.3690 - val_accuracy: 0.5333 - 2s/epoch - 307ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 1.36895 to 1.33730, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.7696 - accuracy: 0.8222 - val_loss: 1.3373 - val_accuracy: 0.5556 - 2s/epoch - 311ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 1.33730 to 1.32339, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.8077 - accuracy: 0.8000 - val_loss: 1.3234 - val_accuracy: 0.5556 - 2s/epoch - 303ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.32339\n",
      "6/6 - 2s - loss: 0.7266 - accuracy: 0.8333 - val_loss: 1.3407 - val_accuracy: 0.5111 - 2s/epoch - 305ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 1.32339 to 1.31270, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.6951 - accuracy: 0.8222 - val_loss: 1.3127 - val_accuracy: 0.5333 - 2s/epoch - 306ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 1.31270 to 1.29241, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.6714 - accuracy: 0.8556 - val_loss: 1.2924 - val_accuracy: 0.5556 - 2s/epoch - 308ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 1.29241 to 1.28462, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.6934 - accuracy: 0.8556 - val_loss: 1.2846 - val_accuracy: 0.5333 - 2s/epoch - 307ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.28462\n",
      "6/6 - 2s - loss: 0.6515 - accuracy: 0.8889 - val_loss: 1.2866 - val_accuracy: 0.5333 - 2s/epoch - 306ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 1.28462 to 1.28212, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.6260 - accuracy: 0.9222 - val_loss: 1.2821 - val_accuracy: 0.5556 - 2s/epoch - 305ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 1.28212 to 1.26942, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.6301 - accuracy: 0.8778 - val_loss: 1.2694 - val_accuracy: 0.5778 - 2s/epoch - 301ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.26942\n",
      "6/6 - 2s - loss: 0.6358 - accuracy: 0.9111 - val_loss: 1.2695 - val_accuracy: 0.5333 - 2s/epoch - 286ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 1.26942 to 1.26899, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.6000 - accuracy: 0.9222 - val_loss: 1.2690 - val_accuracy: 0.5333 - 2s/epoch - 299ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 1.26899 to 1.24495, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.5677 - accuracy: 0.9333 - val_loss: 1.2450 - val_accuracy: 0.5333 - 2s/epoch - 299ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.24495\n",
      "6/6 - 2s - loss: 0.5534 - accuracy: 0.9556 - val_loss: 1.2491 - val_accuracy: 0.5333 - 2s/epoch - 285ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 1.24495 to 1.24000, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.5549 - accuracy: 0.9222 - val_loss: 1.2400 - val_accuracy: 0.5333 - 2s/epoch - 307ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 1.24000 to 1.21828, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.5694 - accuracy: 0.9222 - val_loss: 1.2183 - val_accuracy: 0.5333 - 2s/epoch - 295ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.21828\n",
      "6/6 - 2s - loss: 0.5390 - accuracy: 0.9444 - val_loss: 1.2305 - val_accuracy: 0.5333 - 2s/epoch - 291ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 1.21828 to 1.20841, saving model to /tmp/checkpoint.h5\n",
      "6/6 - 2s - loss: 0.5272 - accuracy: 0.9333 - val_loss: 1.2084 - val_accuracy: 0.5333 - 2s/epoch - 295ms/step\n",
      "2/2 [==============================] - 0s 32ms/step\n",
      "Classification accuracy: 0.622222 \n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# fit the model. Due to very small sample sizes this can get\n",
    "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
    "# Riemannian geometry classification (below)\n",
    "################################################################################\n",
    "fittedModel = model.fit(X_train, y_train, batch_size = 16, epochs = 30, \n",
    "                        verbose = 2, validation_data=(X_validate, y_validate),\n",
    "                        callbacks=[checkpointer], class_weight = class_weights)\n",
    "\n",
    "# load optimal weights\n",
    "model.load_weights('/tmp/checkpoint.h5')\n",
    "\n",
    "###############################################################################\n",
    "# can alternatively used the weights provided in the repo. If so it should get\n",
    "# you 93% accuracy. Change the WEIGHTS_PATH variable to wherever it is on your\n",
    "# system.\n",
    "###############################################################################\n",
    "\n",
    "# WEIGHTS_PATH = /path/to/EEGNet-8-2-weights.h5 \n",
    "# model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "###############################################################################\n",
    "# make prediction on test set.\n",
    "###############################################################################\n",
    "\n",
    "probs       = model.predict(X_test)\n",
    "preds       = probs.argmax(axis = -1)  \n",
    "acc         = np.mean(preds == y_test.argmax(axis=-1))\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 42ms/step\n",
      "Baseline accuracy: 0.622222 \n",
      "x_test.shape:  (45, 64, 656, 1)\n",
      "y_test.shape:  (45, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# evaluate the model\n",
    "probs       = model.predict(X_test)\n",
    "preds       = probs.argmax(axis = -1)  \n",
    "acc         = np.mean(preds == y_test.argmax(axis=-1))\n",
    "print(\"Baseline accuracy: %f \" % (acc))\n",
    "\n",
    "print(\"x_test.shape: \", X_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 77ms/step\n",
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "int_labels_y_test = np.argmax(model.predict(X_test), axis=1)\n",
    "print(int_labels_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. None expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/permutation_importance.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/permutation_importance.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# permutation importance\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/permutation_importance.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m result \u001b[39m=\u001b[39m permutation_importance(model, X_test, y_test, n_repeats\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/permutation_importance.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# get indices of the most important features\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/permutation_importance.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m importances \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mimportances_mean\n",
      "File \u001b[0;32m~/anaconda3/envs/eegnet/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/eegnet/lib/python3.10/site-packages/sklearn/inspection/_permutation_importance.py:264\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Permutation importance for feature evaluation [BRE]_.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39mThe :term:`estimator` is required to be a fitted estimator. `X` can be the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39marray([0.2211..., 0.       , 0.       ])\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 264\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    266\u001b[0m \u001b[39m# Precompute random seed from the random state to be used\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m# to get a fresh independent RandomState instance for each\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m# parallel call to _calculate_permutation_scores, irrespective of\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m# the fact that variables are shared or not depending on the active\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m# joblib backend (sequential, thread-based or process-based).\u001b[39;00m\n\u001b[1;32m    271\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(random_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/eegnet/lib/python3.10/site-packages/sklearn/utils/validation.py:951\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    947\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    948\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    950\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> 951\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m     )\n\u001b[1;32m    956\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    957\u001b[0m     _assert_all_finite(\n\u001b[1;32m    958\u001b[0m         array,\n\u001b[1;32m    959\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    960\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    961\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    962\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. None expected <= 2."
     ]
    }
   ],
   "source": [
    "# permutation importance\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# get indices of the most important features\n",
    "importances = result.importances_mean\n",
    "important_electrodes = result.importances_mean.argsort()[::-1][:16]\n",
    "print(\"16 important electrodes: \", important_electrodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gpt edit: \n",
    "X_2d = np.mean(X, axis=2)\n",
    "y_2d = Y\n",
    "\n",
    "# # split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2d, y_2d, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 64, 656, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 656, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 64).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"model\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 64)\n    \n    Call arguments received by layer \"model\" \"                 f\"(type Functional):\n      • inputs=tf.Tensor(shape=(None, 64), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/permutation_importance.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kaleb/Documents/GitHub/BrainControlledWheelchair/EEG_ML/tests/permutation_importance.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fittedModel \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filerz49ashz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/kaleb/.local/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"model\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 64)\n    \n    Call arguments received by layer \"model\" \"                 f\"(type Functional):\n      • inputs=tf.Tensor(shape=(None, 64), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "fittedModel = model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
